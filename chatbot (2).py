# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Tp11unYtd9kvyS6nM28ubkgfgWDBfQ6
"""

import pandas as pd
import os
import re
from langchain.llms import HuggingFacePipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.document_loaders.csv_loader import CSVLoader
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from huggingface_hub import notebook_login
import torch
import gc
import gradio

notebook_login()

# Global variables
retriever = None
memory = None
llm = None
conversation_chain = None

def load_llm():
    model_id = "mistralai/Mistral-7B-Instruct-v0.3"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    )
    print(model.device)
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.1,
        top_p=0.95,
        repetition_penalty=1.15
    )
    return HuggingFacePipeline(pipeline=pipe), tokenizer

def load_embeddings():
    return HuggingFaceEmbeddings(model_name="multi-qa-MiniLM-L6-cos-v1", model_kwargs={'device': 'cuda'})

def process_csv_data(uploaded_file):
    """Read and process CSV or JSON data, then split it into document chunks."""
    file_extension = os.path.splitext(uploaded_file.name)[-1].lower()

    if file_extension == '.csv':
        df = pd.read_csv(uploaded_file)
    elif file_extension == '.json':
        df = pd.read_json(uploaded_file)
    else:
        return None  # Unsupported file format

    # Convert DataFrame to text
    text_data = "\n".join(df.apply(lambda row: " | ".join(row.dropna().astype(str)), axis=1))

    # Remove consecutive newline characters
    text_data = re.sub(r'\n+', '\n', text_data)

    # Initialize text splitter
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)

    # Process text into LangChain documents
    documents = text_splitter.create_documents([text_data])

    return documents

def create_vectorstore(chunks, embeddings):
  vectore_store = FAISS.from_documents(chunks, embeddings)
  vector_store_path = "faiss_index"
  vectore_store.save_local(vector_store_path)
  vector_store = FAISS.load_local(vector_store_path,
                                  embeddings,
                                  allow_dangerous_deserialization=True
)
  return vectore_store

def setup_chatbot(uploaded_file):
    """Process data and initialize chatbot components."""
    global retriever, memory, llm, conversation_chain

    embeddings = load_embeddings()
    chunks = process_csv_data(uploaded_file)

    if chunks is None:
        return "Unsupported file format. Please upload a CSV or JSON file."

    vectorstore = create_vectorstore(chunks, embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
    llm, _ = load_llm()
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm, retriever=retriever, memory=memory, verbose=False
    )
    return "File processed successfully!"

def chatbot_response(user_input):
    """Generate chatbot response based on user input."""
    s = "Answer this question according to the given context."
    if not conversation_chain:
        return "Please upload a CSV or JSON file first."

    response = conversation_chain.invoke({"question": s + user_input})
    return response["answer"]

# Gradio UI
def gradio_ui():
    """Create and launch Gradio UI."""
    with gradio.Blocks() as demo:
        gradio.Markdown("# Insurance RAG Chatbot")

        with gradio.Row():
            file_input = gradio.File(label="Upload Insurance Data (CSV/JSON)")
            upload_btn = gradio.Button("Process File")

        status_output = gradio.Textbox(label="Status")
        chat_input = gradio.Textbox(label="Ask a question")
        chat_output = gradio.Textbox(label="Bot Response")

        upload_btn.click(setup_chatbot, inputs=[file_input], outputs=[status_output])
        chat_input.submit(chatbot_response, inputs=[chat_input], outputs=[chat_output])

    demo.launch(debug = True, share = True)

if __name__ == "__main__":
    gradio_ui()

